{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaanK2408/CS421_Assignment2/blob/main/translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaOSrvct7dX8"
      },
      "source": [
        "If you're opening this Notebook on colab, you will probably need to install ğŸ¤— Transformers and ğŸ¤— Datasets. Uncomment the following cell and run it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GwJkoNj7dYA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b28b96f-d2f5-4694-ab45-2f6b2b7a34eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchtext\n",
            "  Downloading torchtext-0.18.0-cp311-cp311-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Collecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.3.1)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from torchtext) (2.5.1+cu124)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.3.0->torchtext)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.3.0->torchtext)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.3.0->torchtext)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.3.0->torchtext)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.3.0->torchtext)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.3.0->torchtext)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.3.0->torchtext)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.3.0->torchtext)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.3.0->torchtext)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.3.0->torchtext)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.3.0->torchtext) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.3.0->torchtext) (3.0.2)\n",
            "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchtext-0.18.0-cp311-cp311-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, portalocker, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, dill, colorama, sacrebleu, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets, torchtext, evaluate\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets evaluate sacrebleu torchtext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qhd_GW5J7dYB"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omSuPoLY7dYB"
      },
      "source": [
        "## Q1: Dataset Preparation (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtQHtK5j7dYB"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRQ7Hsrj7dYC"
      },
      "source": [
        "We use the ```load_dataset()``` function to download the dataset. Replace the dummy arguments to download the wmt14 dataset for fr-en translation as provided here: https://huggingface.co/datasets/wmt/wmt14"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klc-jtLi7dYC"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(REPLACE_WITH_DATASET_NAME, REPLACE_WITH_LANGUAGE_PAIR, split='train[:15000]')\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OajGgO-R7dYC"
      },
      "source": [
        "Now, we split the dataset into training and testing splits. This is done using the ```train_test_split``` function. Replace the dummy arguments with appropriate parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oCpCOAv7dYC"
      },
      "outputs": [],
      "source": [
        "split_datasets = dataset.train_test_split(train_size=REPLACE_WITH_TRAIN_SIZE, seed=REPLACE_WITH_SEED)\n",
        "split_datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU3rB0eR7dYD"
      },
      "source": [
        "Define the test dataset as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0KvRQL07dYD"
      },
      "outputs": [],
      "source": [
        "test_dataset = split_datasets[\"test\"]\n",
        "test_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykFRxwh57dYD"
      },
      "source": [
        "Now, follow the same process to split the train dataset to training and validation splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kynutdwu7dYD"
      },
      "outputs": [],
      "source": [
        "split_to_val = YOUR_CODE_HERE\n",
        "train_dataset = YOUR_CODE_HERE\n",
        "eval_dataset = YOUR_CODE_HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J7AypmH7dYD"
      },
      "source": [
        "## Q2 Prepare for training RNNs (10)\n",
        "In this part, you are required to define the tokenizers for english and french, tokenize the data, and define the dataloaders."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaR1qKZs7dYD"
      },
      "source": [
        "Choose and initialize the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfFHDk3D7dYD"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(REPLACE_WITH_MODEL_NAME) # CHOOSE AN APPROPRIATE MULTILINGUAL MODEL such as https://huggingface.co/google-bert/bert-base-multilingual-cased"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITKX-cza7dYD"
      },
      "source": [
        "You will need to create a pytorch dataset to process the tokens in the required format. Complete the implementation of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCKYlKu07dYD"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, dataset, input_size, output_size):\n",
        "        source_texts = [text[\"translation\"][FILL_LANGUAGE] for text in dataset]\n",
        "        target_texts = [text[\"translation\"][FILL_LANGUAGE] for text in dataset]\n",
        "        self.source_sentences = tokenizer(FILL, padding='max_length', truncation=True, return_tensors=\"pt\")[\"input_ids\"]\n",
        "        self.target_sentences = tokenizer(FILL, padding='max_length', truncation=True, return_tensors=\"pt\")[\"input_ids\"]\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.source_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.source_sentences[idx], self.target_sentences[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUl1vQj07dYE"
      },
      "source": [
        "Initialize the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-Hqb62L7dYE"
      },
      "outputs": [],
      "source": [
        "train_dataset_rnn = TranslationDataset(DATASET_SPLIT, vocab_size, vocab_size)\n",
        "eval_dataset_rnn = TranslationDataset(DATASET_SPLIT, vocab_size, vocab_size)\n",
        "test_dataset_rnn = TranslationDataset(DATASET_SPLIT, vocab_size, vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-JfiZAH7dYE"
      },
      "source": [
        "Get the vocab size from the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aR1CEzJN7dYE"
      },
      "outputs": [],
      "source": [
        "vocab_size = tokenizer.vocab_size # This size is used somewhere in the model, think."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8uvrc0R7dYE"
      },
      "source": [
        "Initialize and define the dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8W3qVX2N7dYE"
      },
      "outputs": [],
      "source": [
        "#Instantiate the DataLoaders\n",
        "from torch.utils.data import DataLoader\n",
        "BATCH_SIZE = CHOSEN_BATCH_SIZE\n",
        "train_dataloader = DataLoader(FILL, batch_size=BATCH_SIZE, shuffle=True)\n",
        "eval_dataloader = DataLoader(FILL, batch_size=BATCH_SIZE)\n",
        "test_dataloader = DataLoader(FILL, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X03nj-qd7dYE"
      },
      "source": [
        "## Q3: Implementing RNNs (10)\n",
        "Define the RNN model as an encoder-decoder RNN for the task of translation in the cell below. You may refer: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6iYwjZXt7dYE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZcyOabn7dYE"
      },
      "outputs": [],
      "source": [
        "class Seq2SeqRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "    def forward(self, x):\n",
        "        # YOUR CODE HERE\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnP9NUtK7dYE"
      },
      "outputs": [],
      "source": [
        "model = Seq2SeqRNN(input_size = FILL, hidden_size= FILL, output_size= FILL)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MZb5OZc7dYE"
      },
      "source": [
        "## Q4: Training RNNs (15)\n",
        "In this question, you will define the hyperparameters, loss and optimizer for training. You will then implement a custom training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INYhxibp7dYE"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7cPIUCd7dYE"
      },
      "source": [
        "define the optimizer and the loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FeCoP4DC7dYE"
      },
      "outputs": [],
      "source": [
        "from torch.optim import IMPORT_OPTIMIZER\n",
        "from torch.nn import IMPORT_LOSS_FUNCTION\n",
        "\n",
        "num_train_epochs = NUM_EPOCHS\n",
        "num_training_steps = num_train_epochs * len(train_dataloader)\n",
        "criterion = # YOUR LOSS FUNCTION\n",
        "optimizer = # YOUR OPTIMIZER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2w-eirJ7dYE"
      },
      "source": [
        "Write the training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waBwBdVx7dYE"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "progress_bar = tqdm(total=num_training_steps, desc=\"Training Progress\")\n",
        "\n",
        "for epoch in range(num_train_epochs):\n",
        "    # Training Phase\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_src, batch_tgt in train_dataloader:\n",
        "        ## Complete the training loop\n",
        "\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    # Evaluation Phase\n",
        "    model.eval()\n",
        "\n",
        "    total_batches = 0\n",
        "\n",
        "    for batch_src, batch_tgt in eval_dataloader:\n",
        "\n",
        "\n",
        "      ### Complete the evaluation phase\n",
        "\n",
        "    avg_loss = None\n",
        "    print(f\"Epoch {epoch}: Average Eval Loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-e1x_Lm7dYF"
      },
      "source": [
        "## Q5: Evaluating RNNs for Machine Translation (5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrE3V_P17dYF"
      },
      "source": [
        "Implement the calculation of BLEU-1,2,3,4 scores using the ```sacrebleu``` library for the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xddoMFY17dYF"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "bleu1, bleu2, bleu3, bleu4 = None, None, None, None\n",
        "for batch in test_dataloader:\n",
        "    batch = {k: v.to(model.device) for k, v in batch.items()}\n",
        "    # Complete the testing loop\n",
        "\n",
        "print(\"BLEU-1: \", bleu1)\n",
        "print(\"BLEU-2: \", bleu2)\n",
        "print(\"BLEU-3: \", bleu3)\n",
        "print(\"BLEU-4: \", bleu4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpaHAQWp7dYF"
      },
      "source": [
        "Congratulations! You can now work with RNNs for the task of Machine Translation!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeBy8PO37dYF"
      },
      "source": [
        "## Q6: Prepare for training transformers (10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vauBDs_a7dYJ"
      },
      "source": [
        "In this part we cover the initial setup required before training transformer this including data preprocessing and setting up data collators and loaders.\n",
        "\n",
        "Ensure you have loaded the dataset!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AW5Y83Ew7dYK"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMGCBjca7dYK"
      },
      "source": [
        "We will begin by tokenizing the data. Based on your model selection load the appropriate tokenizer. We are using models from AutoModelForSeq2SeqLM in this assignment. You can checkout all the available models here: https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForSeq2SeqLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qr49H_Bq7dYK"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"\" #Select a model of your choice\n",
        "tokenizer = AutoTokenizer.from_pretrained(REPLACE_WITH_CHECKPOINT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_FwXurd7dYK"
      },
      "source": [
        "We will need to tokenize both our input and outputs. Thus we make use of pre_process() function to generate tokenized model inputs and targets. Ensure you use truncation and padding! The max length will be 128."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KF8_JKSS7dYK"
      },
      "outputs": [],
      "source": [
        "##Implement the preprocess function\n",
        "def preprocess_function(examples):\n",
        "    inputs = [example[SET_RIGHT_LANG] for example in examples[\"translation\"]]\n",
        "    targets = [example[SET_RIGHT_LANG] for example in examples[\"translation\"]]\n",
        "    model_inputs = tokenizer() #Instantitate tokenizer to generate model outputs\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train_data = train_dataset.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "-IwNWQEf8Fru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_val_data = val_dataset.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "H42Bthkh8GHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We remove the column 'translation' as we do not require it for training. Also often having columns other than we created using the preprocess_function may lead to errors during training. Since model might get confused which inputs it needs to use."
      ],
      "metadata": {
        "id": "R9KiU8Uw8jpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train_data = tokenized_train_data.remove_columns(train_dataset.column_names)\n",
        "tokenized_val_data = tokenized_val_data.remove_columns(val_dataset.column_names)"
      ],
      "metadata": {
        "id": "8z_WfysF8Jqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train_data.set_format(\"torch\")\n",
        "tokenized_val_data.set_format(\"torch\")"
      ],
      "metadata": {
        "id": "ES0vpD308KFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBKQon-E7dYK"
      },
      "source": [
        "To construct batches of training data for model training, we require collators that set the properties for the batches and data loaders that generate the batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8J_Bs4bT7dYK"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq() #INSTANTIATE THE COLLATOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ErHyp6A7dYK"
      },
      "outputs": [],
      "source": [
        "#Instantiate the DataLoader for training and evaluation data\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(, batch_size=32, shuffle=True)\n",
        "eval_dataloader = DataLoader(, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rq75QwZr7dYK"
      },
      "source": [
        "## Q7) Choosing & Loading the Model (5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1Cr5Z0T7dYK"
      },
      "source": [
        "Choose a pre-trained transformer model that you will use for fine-tuning on the translation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZY0-m7L7dYK"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(REPLACE_WITH_CHECKPOINT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsaL0EAC7dYK"
      },
      "source": [
        "## Q8) Training the Transformer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKlkWxxh7dYK"
      },
      "source": [
        "Now, that we have are data tokenized and ready in batches and model fixed. We will begin with training this model. To do so we must setup the right hyperparameters, then proceed to implment the training loop to train our model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e0JWoMi7dYL"
      },
      "source": [
        "For training we require an optimizer and a scheduler to manage the learning rate during the training. Let's set them up before our training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxwICUp17dYL"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "from transformers import get_scheduler\n",
        "\n",
        "num_train_epochs = NUM_EPOCHS\n",
        "num_training_steps = NUM_STEPS\n",
        "\n",
        "optimizer = SETUP_Adam_OPTIMIZER\n",
        "lr_scheduler = SETUP_SCHEDULER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqdgWb_Q7dYL"
      },
      "source": [
        "Finally, we are here!\n",
        "\n",
        "In the loop during training you will run a forward pass, compute the loss, compute the gradients, and then update the weights. (Don't foregt to set gradient to zero!)\n",
        "\n",
        "During the eval phase we simply do a forward pass and compute the loss!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0iDbp0x7dYL"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "progress_bar = tqdm(total=num_training_steps, desc=\"Training Progress\")\n",
        "\n",
        "for epoch in range(num_train_epochs):\n",
        "    # Training Phase\n",
        "    model.train()\n",
        "    for batch in train_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        ## Complete the training loop\n",
        "\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    # Evaluation Phase\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_batches = 0\n",
        "\n",
        "    for batch in eval_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "      ### Complete the evaluation phase\n",
        "\n",
        "    avg_loss = None\n",
        "    print(f\"Epoch {epoch}: Average Eval Loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_7JA8gD7dYL"
      },
      "source": [
        "Congratulations!! On completing the training. Now don't forget to save your model and the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqTmBLMG7dYL"
      },
      "outputs": [],
      "source": [
        "# Save model and tokenizer\n",
        "output_dir = SET_OUTPUT_DIR\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCrwRPf-7dYL"
      },
      "source": [
        "## Q9) Evaluating Transformer for Machine Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsteZEw77dYL"
      },
      "source": [
        "We will now test our trained model and analyze its performance using BLEU-1, 2, 3, 4 scores from the sacrebleu library. You will create a task evaluator for translation, load and process the test dataset, and compute the results on an existing trained model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we load a model trained for french to english translation. You can read more about it here: https://huggingface.co/Helsinki-NLP/opus-mt-tc-big-fr-en"
      ],
      "metadata": {
        "id": "YNP8tJ3Y9Yna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "checkpoint = \"Helsinki-NLP/opus-mt-tc-big-fr-en\"\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "OUIYns7E9S2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize an evaluator for translation task"
      ],
      "metadata": {
        "id": "1HRtKCKh9mqc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dR_Kc8NN7dYL"
      },
      "outputs": [],
      "source": [
        "## Load Evaluator for translation\n",
        "from evaluate import evaluator\n",
        "task_evaluator = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qL8zPaFz7dYL"
      },
      "source": [
        "We will need to change our test dataset by having specific input and target columns. Thus we will use split_translation to split the translation column into two columns 'en' and 'fr'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMCG74Vr7dYL"
      },
      "outputs": [],
      "source": [
        "#  Implement the split function\n",
        "def split_translations(example):\n",
        "    en_text = example[][]\n",
        "    fr_text = example[][]\n",
        "    example['en'] =\n",
        "    example['fr'] =\n",
        "    return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpUlKUm17dYL"
      },
      "outputs": [],
      "source": [
        "test_data = test_dataset.map(split_translations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oA9Vt7kF7dYL"
      },
      "source": [
        "You can now go ahead and compute the results by appropriately setting up the task_evaluator.compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HV6gQhFP7dYL"
      },
      "outputs": [],
      "source": [
        "results = task_evaluator.compute(\n",
        "    model_or_pipeline= MODEL,\n",
        "    data= DATA,\n",
        "    metric=METRIC,\n",
        "    input_column=COLUMN,\n",
        "    label_column=COLUMN,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEv4BBNq7dYM"
      },
      "outputs": [],
      "source": [
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvPJif197dYM"
      },
      "source": [
        "## Q10) Inferencing on Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2VpQtGW7dYM"
      },
      "source": [
        "Let's check out how well this trained model's translation skills are. You can use try with a few french sentence and see how well it translates.\n",
        "\n",
        "To do so we will setup a pipline using the existing trained model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6mVfvWC7dYM"
      },
      "source": [
        "Loading the tokenizer and model for the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nojJlgVf7dYM"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "checkpoint = \"Helsinki-NLP/opus-mt-tc-big-fr-en\"\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53K25pUn7dYM"
      },
      "source": [
        "Setup the pipeline for translation using your model and tokenizer. You can read about pipelines here: https://huggingface.co/docs/transformers/en/main_classes/pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKp9pNoQ7dYM"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "# Instatiate a pipeline for Translation using the model and tokenizer\n",
        "pipeline = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdV-e6sM7dYM"
      },
      "source": [
        "Translate the given sentence using the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tc8Y01YC7dYM"
      },
      "outputs": [],
      "source": [
        "input_text = \"REPLACE WITH A SENTENCE IN FRENCH.\"\n",
        "translation_result = pipeline(REPLACE_WITH_TEXT_TO_TRANSLATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eRCXrNN7dYM"
      },
      "outputs": [],
      "source": [
        "print(translation_result)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}