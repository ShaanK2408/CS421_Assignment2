{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaanK2408/CS421_Assignment2/blob/main/translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaOSrvct7dX8"
      },
      "source": [
        "If you're opening this Notebook on colab, you will probably need to install ðŸ¤— Transformers and ðŸ¤— Datasets. Uncomment the following cell and run it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8GwJkoNj7dYA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "aa573d64-0d75-410e-ab77-b4001ffc47df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.11/dist-packages (2.5.1)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.11/dist-packages (0.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (3.1.1)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.3.1)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from torchtext) (2.5.1+cu124)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.3.0->torchtext) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.3.0->torchtext) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets evaluate sacrebleu torchtext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Qhd_GW5J7dYB"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omSuPoLY7dYB"
      },
      "source": [
        "## Q1: Dataset Preparation (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VtQHtK5j7dYB"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRQ7Hsrj7dYC"
      },
      "source": [
        "We use the ```load_dataset()``` function to download the dataset. Replace the dummy arguments to download the wmt14 dataset for fr-en translation as provided here: https://huggingface.co/datasets/wmt/wmt14"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "klc-jtLi7dYC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223,
          "referenced_widgets": [
            "40133ea5161646caab519ae379ca798c",
            "9353c45ca41c42c59e7e30fcf6b35fa6",
            "f9736e8e0a7e4404857ac8778a251aee",
            "55101ceacb5342de98ec9a201f3020a7",
            "eb91051d7d1a4f0281ac9dd5534056fa",
            "a6b201ba6bd64d44a862cc9f7f82a029",
            "927a96a526644b7990c0e43d0db1c7a5",
            "14bb070debba4f84b7379e5eb3037b47",
            "135b76f793d44810b9f0cb638bfa3d2d",
            "0fa84473e42d4d11880478859d10df56",
            "01c64c7cf7254e959388dba92bf2f7d1"
          ]
        },
        "outputId": "e831cb97-fd4c-467c-c160-2b793e98cff7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "40133ea5161646caab519ae379ca798c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['translation'],\n",
              "    num_rows: 15000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "dataset = load_dataset(\"wmt14\", \"fr-en\", split='train[:15000]')\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OajGgO-R7dYC"
      },
      "source": [
        "Now, we split the dataset into training and testing splits. This is done using the ```train_test_split``` function. Replace the dummy arguments with appropriate parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9oCpCOAv7dYC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0671ed9-10d6-47e0-883b-ee179c561642"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['translation'],\n",
              "        num_rows: 12000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['translation'],\n",
              "        num_rows: 3000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "split_datasets = dataset.train_test_split(train_size=0.8, seed=42)\n",
        "split_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU3rB0eR7dYD"
      },
      "source": [
        "Define the test dataset as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "F0KvRQL07dYD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a373c862-8fae-4435-9772-0aab6aa73d81"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['translation'],\n",
              "    num_rows: 3000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "test_dataset = split_datasets[\"test\"]\n",
        "test_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykFRxwh57dYD"
      },
      "source": [
        "Now, follow the same process to split the train dataset to training and validation splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kynutdwu7dYD"
      },
      "outputs": [],
      "source": [
        "split_to_val = split_datasets[\"train\"].train_test_split(train_size=0.875, seed=42)\n",
        "train_dataset = split_to_val[\"train\"]\n",
        "eval_dataset = split_to_val[\"test\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J7AypmH7dYD"
      },
      "source": [
        "## Q2 Prepare for training RNNs (10)\n",
        "In this part, you are required to define the tokenizers for english and french, tokenize the data, and define the dataloaders."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaR1qKZs7dYD"
      },
      "source": [
        "Choose and initialize the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qfFHDk3D7dYD"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\") # CHOOSE AN APPROPRIATE MULTILINGUAL MODEL such as https://huggingface.co/google-bert/bert-base-multilingual-cased"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITKX-cza7dYD"
      },
      "source": [
        "You will need to create a pytorch dataset to process the tokens in the required format. Complete the implementation of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "uCKYlKu07dYD"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, dataset, input_size, output_size):\n",
        "        source_texts = [text[\"translation\"][\"fr\"] for text in dataset]\n",
        "        target_texts = [text[\"translation\"][\"en\"] for text in dataset]\n",
        "        self.source_sentences = tokenizer(source_texts, padding='max_length', truncation=True, return_tensors=\"pt\")[\"input_ids\"]\n",
        "        self.target_sentences = tokenizer(target_texts, padding='max_length', truncation=True, return_tensors=\"pt\")[\"input_ids\"]\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.source_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.source_sentences[idx], self.target_sentences[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUl1vQj07dYE"
      },
      "source": [
        "Initialize the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "u-Hqb62L7dYE"
      },
      "outputs": [],
      "source": [
        "train_dataset_rnn = TranslationDataset(train_dataset, vocab_size, vocab_size)\n",
        "eval_dataset_rnn = TranslationDataset(eval_dataset, vocab_size, vocab_size)\n",
        "test_dataset_rnn = TranslationDataset(test_dataset, vocab_size, vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-JfiZAH7dYE"
      },
      "source": [
        "Get the vocab size from the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "aR1CEzJN7dYE"
      },
      "outputs": [],
      "source": [
        "vocab_size = tokenizer.vocab_size # This size is used somewhere in the model, think."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8uvrc0R7dYE"
      },
      "source": [
        "Initialize and define the dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "8W3qVX2N7dYE"
      },
      "outputs": [],
      "source": [
        "#Instantiate the DataLoaders\n",
        "from torch.utils.data import DataLoader\n",
        "BATCH_SIZE = 4\n",
        "train_dataloader = DataLoader(train_dataset_rnn, batch_size=BATCH_SIZE, shuffle=True)\n",
        "eval_dataloader = DataLoader(eval_dataset_rnn, batch_size=BATCH_SIZE)\n",
        "test_dataloader = DataLoader(test_dataset_rnn, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X03nj-qd7dYE"
      },
      "source": [
        "## Q3: Implementing RNNs (10)\n",
        "Define the RNN model as an encoder-decoder RNN for the task of translation in the cell below. You may refer: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "6iYwjZXt7dYE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "kZcyOabn7dYE"
      },
      "outputs": [],
      "source": [
        "class Seq2SeqRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "\n",
        "        # Embedding layer and RNN for source language (encoder)\n",
        "        self.encoder_embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.encoder_rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)\n",
        "\n",
        "        # Decoding layer and RNN for target language (decoder)\n",
        "        self.decoder_embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.decoder_rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)\n",
        "\n",
        "        # Fully connected layer to map decoder hidden states to target vocab\n",
        "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        source_tokens, target_tokens = x\n",
        "\n",
        "        # Embeds source tokens and passes it through encoder RNN\n",
        "        embedded_source = self.encoder_embedding(source_tokens)\n",
        "        _, hidden = self.encoder_rnn(embedded_source)\n",
        "\n",
        "        # Embeds target tokens and passes it through decoder RNN\n",
        "        embedded_target = self.decoder_embedding(target_tokens)\n",
        "        output, _ = self.decoder_rnn(embedded_target, hidden)\n",
        "\n",
        "        output = self.fc_out(output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "SnP9NUtK7dYE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a6738ec-7507-44de-d224-2603097c0c74"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2SeqRNN(\n",
              "  (encoder_embedding): Embedding(119547, 64)\n",
              "  (encoder_rnn): RNN(64, 64, batch_first=True)\n",
              "  (decoder_embedding): Embedding(119547, 64)\n",
              "  (decoder_rnn): RNN(64, 64, batch_first=True)\n",
              "  (fc_out): Linear(in_features=64, out_features=119547, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "model = Seq2SeqRNN(input_size = vocab_size, hidden_size= 64, output_size= vocab_size)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MZb5OZc7dYE"
      },
      "source": [
        "## Q4: Training RNNs (15)\n",
        "In this question, you will define the hyperparameters, loss and optimizer for training. You will then implement a custom training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "INYhxibp7dYE"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7cPIUCd7dYE"
      },
      "source": [
        "define the optimizer and the loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "FeCoP4DC7dYE"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "num_train_epochs = 5\n",
        "num_training_steps = num_train_epochs * len(train_dataloader)\n",
        "criterion = CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "optimizer = Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2w-eirJ7dYE"
      },
      "source": [
        "Write the training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "waBwBdVx7dYE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26c17244-be56-4094-a6b7-a9ae5c0c36aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   0%|          | 0/1645 [00:57<?, ?it/s]\n",
            "Training Progress:  20%|â–ˆâ–ˆ        | 2626/13125 [04:00<6:09:12,  2.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Average Eval Loss: 0.5917\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 5251/13125 [08:00<5:23:44,  2.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Average Eval Loss: 0.2528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 7876/13125 [12:00<3:35:52,  2.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Average Eval Loss: 0.1717\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 10501/13125 [16:01<1:47:52,  2.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Average Eval Loss: 0.1450\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13125/13125 [19:48<00:00, 11.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Average Eval Loss: 0.1323\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "progress_bar = tqdm(total=num_training_steps, desc=\"Training Progress\")\n",
        "\n",
        "for epoch in range(num_train_epochs):\n",
        "    # Training Phase\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_src, batch_tgt in train_dataloader:\n",
        "        ## Complete the training loop\n",
        "\n",
        "        # Move data to GPU\n",
        "        if torch.cuda.is_available():\n",
        "            batch_src = batch_src.cuda()\n",
        "            batch_tgt = batch_tgt.cuda()\n",
        "\n",
        "        # Reset gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        output = model((batch_src, batch_tgt))\n",
        "\n",
        "        # Reshape for CrossEntropyLoss: [batch_size * seq_len, vocab_size]\n",
        "        output_reshaped = output.view(-1, output.size(-1))\n",
        "        target_reshaped = batch_tgt.view(-1)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(output_reshaped, target_reshaped)\n",
        "\n",
        "        # Backward propoagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    # Evaluation Phase\n",
        "    model.eval()\n",
        "\n",
        "    total_batches = 0\n",
        "    total_eval_loss = 0.0 # Added in order to accumulate eval loss\n",
        "\n",
        "    for batch_src, batch_tgt in eval_dataloader:\n",
        "      ### Complete the evaluation phase\n",
        "\n",
        "      with torch.no_grad():\n",
        "        if torch.cuda.is_available():\n",
        "            batch_src = batch_src.cuda()\n",
        "            batch_tgt = batch_tgt.cuda()\n",
        "\n",
        "        # Forward pass\n",
        "        output = model((batch_src, batch_tgt))\n",
        "        output_reshaped = output.view(-1, output.size(-1))\n",
        "        target_reshaped = batch_tgt.view(-1)\n",
        "\n",
        "        # Compute loss\n",
        "        eval_loss = criterion(output_reshaped, target_reshaped)\n",
        "        total_eval_loss += eval_loss.item()\n",
        "        total_batches += 1\n",
        "\n",
        "    avg_loss = total_eval_loss / total_batches if total_batches > 0 else 0\n",
        "    print(f\"Epoch {epoch}: Average Eval Loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-e1x_Lm7dYF"
      },
      "source": [
        "## Q5: Evaluating RNNs for Machine Translation (5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrE3V_P17dYF"
      },
      "source": [
        "Implement the calculation of BLEU-1,2,3,4 scores using the ```sacrebleu``` library for the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "xddoMFY17dYF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cad1627-0cd0-4845-f3ab-9f37f91ddc37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU-1:  5.481205885021917\n",
            "BLEU-2:  5.366024942498578\n",
            "BLEU-3:  5.2521330601423974\n",
            "BLEU-4:  5.138762277109498\n"
          ]
        }
      ],
      "source": [
        "import sacrebleu\n",
        "from sacrebleu.metrics import BLEU\n",
        "\n",
        "model.eval()\n",
        "bleu1, bleu2, bleu3, bleu4 = None, None, None, None\n",
        "\n",
        "predictions, references = [], []\n",
        "\n",
        "for batch in test_dataloader:\n",
        "    batch = {\"source\": batch[0], \"target\": batch[1]}\n",
        "\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "    output = model((batch[\"source\"], batch[\"target\"]))\n",
        "    pred_ids = output.argmax(dim=-1)\n",
        "\n",
        "    for i in range(pred_ids.size(0)):\n",
        "        pred_text = tokenizer.decode(pred_ids[i].cpu(), skip_special_tokens=True)\n",
        "        ref_text  = tokenizer.decode(batch[\"target\"][i].cpu(), skip_special_tokens=True)\n",
        "        predictions.append(pred_text)\n",
        "        references.append(ref_text)\n",
        "\n",
        "# BLEU for 1-gram\n",
        "bleu1_metric = BLEU(max_ngram_order=1)\n",
        "bleu1_score = bleu1_metric.corpus_score(predictions, [references])\n",
        "bleu1 = bleu1_score.score\n",
        "\n",
        "# BLEU for 1-2 grams\n",
        "bleu2_metric = BLEU(max_ngram_order=2)\n",
        "bleu2_score = bleu2_metric.corpus_score(predictions, [references])\n",
        "bleu2 = bleu2_score.score\n",
        "\n",
        "# BLEU for 1-3 grams\n",
        "bleu3_metric = BLEU(max_ngram_order=3)\n",
        "bleu3_score = bleu3_metric.corpus_score(predictions, [references])\n",
        "bleu3 = bleu3_score.score\n",
        "\n",
        "# BLEU for 1-4 grams\n",
        "bleu4_metric = BLEU(max_ngram_order=4)\n",
        "bleu4_score = bleu4_metric.corpus_score(predictions, [references])\n",
        "bleu4 = bleu4_score.score\n",
        "\n",
        "print(\"BLEU-1: \", bleu1)\n",
        "print(\"BLEU-2: \", bleu2)\n",
        "print(\"BLEU-3: \", bleu3)\n",
        "print(\"BLEU-4: \", bleu4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpaHAQWp7dYF"
      },
      "source": [
        "Congratulations! You can now work with RNNs for the task of Machine Translation!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeBy8PO37dYF"
      },
      "source": [
        "## Q6: Prepare for training transformers (10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vauBDs_a7dYJ"
      },
      "source": [
        "In this part we cover the initial setup required before training transformer this including data preprocessing and setting up data collators and loaders.\n",
        "\n",
        "Ensure you have loaded the dataset!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AW5Y83Ew7dYK"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMGCBjca7dYK"
      },
      "source": [
        "We will begin by tokenizing the data. Based on your model selection load the appropriate tokenizer. We are using models from AutoModelForSeq2SeqLM in this assignment. You can checkout all the available models here: https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForSeq2SeqLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qr49H_Bq7dYK"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"\" #Select a model of your choice\n",
        "tokenizer = AutoTokenizer.from_pretrained(REPLACE_WITH_CHECKPOINT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_FwXurd7dYK"
      },
      "source": [
        "We will need to tokenize both our input and outputs. Thus we make use of pre_process() function to generate tokenized model inputs and targets. Ensure you use truncation and padding! The max length will be 128."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KF8_JKSS7dYK"
      },
      "outputs": [],
      "source": [
        "##Implement the preprocess function\n",
        "def preprocess_function(examples):\n",
        "    inputs = [example[SET_RIGHT_LANG] for example in examples[\"translation\"]]\n",
        "    targets = [example[SET_RIGHT_LANG] for example in examples[\"translation\"]]\n",
        "    model_inputs = tokenizer() #Instantitate tokenizer to generate model outputs\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train_data = train_dataset.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "-IwNWQEf8Fru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_val_data = val_dataset.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "H42Bthkh8GHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We remove the column 'translation' as we do not require it for training. Also often having columns other than we created using the preprocess_function may lead to errors during training. Since model might get confused which inputs it needs to use."
      ],
      "metadata": {
        "id": "R9KiU8Uw8jpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train_data = tokenized_train_data.remove_columns(train_dataset.column_names)\n",
        "tokenized_val_data = tokenized_val_data.remove_columns(val_dataset.column_names)"
      ],
      "metadata": {
        "id": "8z_WfysF8Jqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train_data.set_format(\"torch\")\n",
        "tokenized_val_data.set_format(\"torch\")"
      ],
      "metadata": {
        "id": "ES0vpD308KFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBKQon-E7dYK"
      },
      "source": [
        "To construct batches of training data for model training, we require collators that set the properties for the batches and data loaders that generate the batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8J_Bs4bT7dYK"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq() #INSTANTIATE THE COLLATOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ErHyp6A7dYK"
      },
      "outputs": [],
      "source": [
        "#Instantiate the DataLoader for training and evaluation data\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(, batch_size=32, shuffle=True)\n",
        "eval_dataloader = DataLoader(, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rq75QwZr7dYK"
      },
      "source": [
        "## Q7) Choosing & Loading the Model (5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1Cr5Z0T7dYK"
      },
      "source": [
        "Choose a pre-trained transformer model that you will use for fine-tuning on the translation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZY0-m7L7dYK"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(REPLACE_WITH_CHECKPOINT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsaL0EAC7dYK"
      },
      "source": [
        "## Q8) Training the Transformer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKlkWxxh7dYK"
      },
      "source": [
        "Now, that we have are data tokenized and ready in batches and model fixed. We will begin with training this model. To do so we must setup the right hyperparameters, then proceed to implment the training loop to train our model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e0JWoMi7dYL"
      },
      "source": [
        "For training we require an optimizer and a scheduler to manage the learning rate during the training. Let's set them up before our training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxwICUp17dYL"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "from transformers import get_scheduler\n",
        "\n",
        "num_train_epochs = NUM_EPOCHS\n",
        "num_training_steps = NUM_STEPS\n",
        "\n",
        "optimizer = SETUP_Adam_OPTIMIZER\n",
        "lr_scheduler = SETUP_SCHEDULER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqdgWb_Q7dYL"
      },
      "source": [
        "Finally, we are here!\n",
        "\n",
        "In the loop during training you will run a forward pass, compute the loss, compute the gradients, and then update the weights. (Don't foregt to set gradient to zero!)\n",
        "\n",
        "During the eval phase we simply do a forward pass and compute the loss!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0iDbp0x7dYL"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "progress_bar = tqdm(total=num_training_steps, desc=\"Training Progress\")\n",
        "\n",
        "for epoch in range(num_train_epochs):\n",
        "    # Training Phase\n",
        "    model.train()\n",
        "    for batch in train_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        ## Complete the training loop\n",
        "\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    # Evaluation Phase\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_batches = 0\n",
        "\n",
        "    for batch in eval_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "      ### Complete the evaluation phase\n",
        "\n",
        "    avg_loss = None\n",
        "    print(f\"Epoch {epoch}: Average Eval Loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_7JA8gD7dYL"
      },
      "source": [
        "Congratulations!! On completing the training. Now don't forget to save your model and the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqTmBLMG7dYL"
      },
      "outputs": [],
      "source": [
        "# Save model and tokenizer\n",
        "output_dir = SET_OUTPUT_DIR\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCrwRPf-7dYL"
      },
      "source": [
        "## Q9) Evaluating Transformer for Machine Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsteZEw77dYL"
      },
      "source": [
        "We will now test our trained model and analyze its performance using BLEU-1, 2, 3, 4 scores from the sacrebleu library. You will create a task evaluator for translation, load and process the test dataset, and compute the results on an existing trained model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we load a model trained for french to english translation. You can read more about it here: https://huggingface.co/Helsinki-NLP/opus-mt-tc-big-fr-en"
      ],
      "metadata": {
        "id": "YNP8tJ3Y9Yna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "checkpoint = \"Helsinki-NLP/opus-mt-tc-big-fr-en\"\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "OUIYns7E9S2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize an evaluator for translation task"
      ],
      "metadata": {
        "id": "1HRtKCKh9mqc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dR_Kc8NN7dYL"
      },
      "outputs": [],
      "source": [
        "## Load Evaluator for translation\n",
        "from evaluate import evaluator\n",
        "task_evaluator = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qL8zPaFz7dYL"
      },
      "source": [
        "We will need to change our test dataset by having specific input and target columns. Thus we will use split_translation to split the translation column into two columns 'en' and 'fr'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMCG74Vr7dYL"
      },
      "outputs": [],
      "source": [
        "#  Implement the split function\n",
        "def split_translations(example):\n",
        "    en_text = example[][]\n",
        "    fr_text = example[][]\n",
        "    example['en'] =\n",
        "    example['fr'] =\n",
        "    return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpUlKUm17dYL"
      },
      "outputs": [],
      "source": [
        "test_data = test_dataset.map(split_translations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oA9Vt7kF7dYL"
      },
      "source": [
        "You can now go ahead and compute the results by appropriately setting up the task_evaluator.compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HV6gQhFP7dYL"
      },
      "outputs": [],
      "source": [
        "results = task_evaluator.compute(\n",
        "    model_or_pipeline= MODEL,\n",
        "    data= DATA,\n",
        "    metric=METRIC,\n",
        "    input_column=COLUMN,\n",
        "    label_column=COLUMN,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEv4BBNq7dYM"
      },
      "outputs": [],
      "source": [
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvPJif197dYM"
      },
      "source": [
        "## Q10) Inferencing on Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2VpQtGW7dYM"
      },
      "source": [
        "Let's check out how well this trained model's translation skills are. You can use try with a few french sentence and see how well it translates.\n",
        "\n",
        "To do so we will setup a pipline using the existing trained model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6mVfvWC7dYM"
      },
      "source": [
        "Loading the tokenizer and model for the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nojJlgVf7dYM"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "checkpoint = \"Helsinki-NLP/opus-mt-tc-big-fr-en\"\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53K25pUn7dYM"
      },
      "source": [
        "Setup the pipeline for translation using your model and tokenizer. You can read about pipelines here: https://huggingface.co/docs/transformers/en/main_classes/pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKp9pNoQ7dYM"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "# Instatiate a pipeline for Translation using the model and tokenizer\n",
        "pipeline = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdV-e6sM7dYM"
      },
      "source": [
        "Translate the given sentence using the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tc8Y01YC7dYM"
      },
      "outputs": [],
      "source": [
        "input_text = \"REPLACE WITH A SENTENCE IN FRENCH.\"\n",
        "translation_result = pipeline(REPLACE_WITH_TEXT_TO_TRANSLATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eRCXrNN7dYM"
      },
      "outputs": [],
      "source": [
        "print(translation_result)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "40133ea5161646caab519ae379ca798c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9353c45ca41c42c59e7e30fcf6b35fa6",
              "IPY_MODEL_f9736e8e0a7e4404857ac8778a251aee",
              "IPY_MODEL_55101ceacb5342de98ec9a201f3020a7"
            ],
            "layout": "IPY_MODEL_eb91051d7d1a4f0281ac9dd5534056fa"
          }
        },
        "9353c45ca41c42c59e7e30fcf6b35fa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6b201ba6bd64d44a862cc9f7f82a029",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_927a96a526644b7990c0e43d0db1c7a5",
            "value": "Resolvingâ€‡dataâ€‡files:â€‡100%"
          }
        },
        "f9736e8e0a7e4404857ac8778a251aee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14bb070debba4f84b7379e5eb3037b47",
            "max": 30,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_135b76f793d44810b9f0cb638bfa3d2d",
            "value": 30
          }
        },
        "55101ceacb5342de98ec9a201f3020a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fa84473e42d4d11880478859d10df56",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_01c64c7cf7254e959388dba92bf2f7d1",
            "value": "â€‡30/30â€‡[00:00&lt;00:00,â€‡â€‡4.62it/s]"
          }
        },
        "eb91051d7d1a4f0281ac9dd5534056fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6b201ba6bd64d44a862cc9f7f82a029": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "927a96a526644b7990c0e43d0db1c7a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "14bb070debba4f84b7379e5eb3037b47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "135b76f793d44810b9f0cb638bfa3d2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0fa84473e42d4d11880478859d10df56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01c64c7cf7254e959388dba92bf2f7d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}